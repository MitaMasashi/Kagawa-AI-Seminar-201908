{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "<h1 align=\"center\"><font color=\"#5831bc\" face=\"Comic Sans MS\">An Insightful Story About Successful Crowdfunding Projects</font></h1> \n",
    "<br>\n",
    "\n",
    "***Crowdfunding*** is the practice of funding a project or a venture by raising monetary contributions from many people across the globe. There are a number of organisations such as DonorsChoose.org, Patreon, Kickstarter which hosts the crowdfunding projects on their platforms. Kickstarter has hosted more than 250,000 projects on their website with more than $4 Billion collective amount raised.  \n",
    "<br>\n",
    "\n",
    "![](https://people.safecreative.org/viewimage/viewmagazine?path=kickstarter-patreon.jpg&defaultPath=)\n",
    "\n",
    "<br>\n",
    "While it is true that crowdfunding is one of the most popular methods to to raise funds however the reality is that **not every project is able to completely reach the goal**. Infact, on KickStarter, only about 35 percent of the total projects have raised successful fundings in the past. This fact raises an important question - **which projects are able to successfully achieve their goal?**. In other words, can project owners somehow know what are the key project characteristics that increases the chances of success. \n",
    "\n",
    "In many studies, Researchers and analysts have used the descriptive analysis methods on the crowdfunding data to obtain insights related to project success. While many others have also applied predictive modelling to obtain the probability of project success. However, these approaches have the fundamental problems: \n",
    "\n",
    "- Descriptive analysis - only gives surface level insights    \n",
    "- Predictive analysis - models act as the blackboxes    \n",
    "\n",
    "### About this Kernel  \n",
    "In this kernel, I have shared a hybrid analysis approach that uses the concepts of both types of analysis enriched with the concepts of **machine learning explainability** which can be used to answer the key questions related to the success (or failure) of any crowdfunding project. The framework uses the interpretations derived from a trained machine learning model. Unlike descriptive analysis to find key insights, the focus in this approach is to make use of model behaviours and characteristics such as : Relative Feature Importances, Partial Dependencies, Permutation Importances, SHAP values. I have explained the intuition behind every approach in layman terms. Following are the contents of the kernel: \n",
    "\n",
    "## <font color=\"#5831bc\" face=\"Comic Sans MS\">Contents</font> \n",
    "\n",
    "1. <a href=\"#1\">Business Use-Case and Problem Statement</a>    \n",
    "2. <a href=\"#2\">Hypothesis Generation</a>    \n",
    "3. <a href=\"#3\">Dataset Preparation</a>     \n",
    "4. <a href=\"#4\">Modelling the Project Success</a>       \n",
    "5. <a href=\"#5\">Model Interpretation : Insights Generation</a>    \n",
    "    <a href=\"#5-1\">5.1 Which are the most important features (relatively) of a project? ( **Relative Feature Importance** )</a>     \n",
    "    <a href=\"#5-2\">5.2 Which features have the biggest impact on the project success? ( **Permutation Importance** )</a>     \n",
    "    <a href=\"#5-3\">5.3 How does changes in those features affact the project success? ( **Partial Dependencies** )</a>     \n",
    "    <a href=\"#5-4\">5.4 Digging deeper into the decisions made by the model ( **SHAP values** )</a>    \n",
    "6. <a href=\"#6\">Final Conclusions</a>       \n",
    "  \n",
    "<div id=\"1\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">1. Understanding the Business Use Case</font>   </h2><br>\n",
    "The essential business use-cases in the crowdfunding scenario can be considered from two different perspectives - from the project owner's perspective and the companies perspective. \n",
    "\n",
    "1. From the **project owner's perspective**, it is highly beneficial to be aware about the key characteristics of a project that greatly influence the success of any project. For instance, it will be interesting to pre-emptively know about following questions:   \n",
    "\n",
    "     - What is an ideal and optimal range of the funding goal for my project ?  \n",
    "     - On which day of the week, I should post the project on Kickstarter ?  \n",
    "     - How many keywords should I use in my project title ?  \n",
    "     - What should be the total length of my project description ?     \n",
    "\n",
    "\n",
    "2. From the **perspective of companies** which hosts the crowdfunding projects such as DonorsChoose.org, Patreon, and Kickstarter, they receive hundreds of thousands of project proposals every year. A large amount of manual effort is required to screen the project before it is approved to be hosted on the platform. This creates the challenges related to scalability, consistency of project vetting across volunteers, and identification of projects which require special assistance. \n",
    "\n",
    "It is due to these two perspectives, there is a need to dig deeper and find more intutive insights related to the projects success. Using these insights, more people can get their projects funded more quickly, and with less cost to the hosting companies. This also allows the hosting companies to optimize the processes and channel even more funding directly to projects.   \n",
    "\n",
    "<div id=\"2\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">2. Hypothesis Generation </font>   </h2> <br>\n",
    "Hypothesis Generation is very powerful technique which can help an analyst to structure a very insightful and a relevant solution of a business problem. It is a process of building an intuitive approach of the business problem without even thinking about the available data. Whenever I start with any new business problem, I try to make a comprehensive list of all the factors which can be used to obtain the final output. For example, which features should affect my predictions. Or, which values of those features will give me the best possible result. In case of crowdfunding, the question can be - which features are very important to decide if a project will be successful or not.  \n",
    "\n",
    "So, to generate the hypothesis for the use-case, we will write down a list of factors (without even looking at the available data) that can possibly be important to model the project success.   \n",
    "\n",
    "1. **Total amount to be raised** - More amount may decrease the chances that the project will be successful.  \n",
    "2. **Total duration of the project** - It is possible that projects which are active for very short or very long time periods are not successful.  \n",
    "3. **Theme of the project** - People may consider donating to a project which has a good cause or a good theme.  \n",
    "4. **Writing style of the project description** - If the message is not very clear, the project may not get complete funding.  \n",
    "5. **Length of the project description** - Very long piecies of text may not perform good as compared to shorter crisp texts.  \n",
    "6. **Project launch time** - A project launched on weekdays as compared to weekends or holidays may not get complete funding amount.  \n",
    "\n",
    "So this is an incomplete list of possible factors we can think at this stage that may influence the project success. Now, using machine learning interpretability, not only we can try to understand which features are actually important but also what are the feature values which these features can take. \n",
    "\n",
    "<div id=\"3\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">3. Dataset Preparation </font>   </h2>\n",
    "<br>Our business use-case is identified, problem statement is formulated, and we have defined a hypothesis. We can now start the analysis, modelling, and interpretting in order to find out the key insights. First, we load the available dataset. \n",
    "\n",
    "### <font color=\"#5831bc\" face=\"Comic Sans MS\">3.1 Load Dataset</font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c76367960e92c4819776cd646556fe14213ca27f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdpbox'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2bd36fc244c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpdpbox\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpdp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo_plots\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0meli5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdpbox'"
     ]
    }
   ],
   "source": [
    "## load required libraries \n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import warnings\n",
    "import eli5\n",
    "warnings.filterwarnings('ignore')\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith(\"e\"):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "projects = pd.read_csv(\"../input/ks-projects-201801.csv\", parse_dates = [\"launched\", \"deadline\"])\n",
    "\n",
    "print (\"Total Projects: \", projects.shape[0], \"\\nTotal Features: \", projects.shape[1])\n",
    "projects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d95c7c17009458684d7ead7f1d4004f3e6de9ba9"
   },
   "source": [
    "### <font color=\"#5831bc\" face=\"Comic Sans MS\">3.2 Dataset Preprocessing</font>  \n",
    "\n",
    "In this dataset, we can see that a number of features are about the active stage of the project. This means that a project was launched on a particular date and a partial amount is already raised. The goal of our problem statement is a little bit different, we want to focus on the stage in which the project is not launched yet and identify if it will successful or not. Additinaly, find the most important features (and the feature values) that influence this output. So we perform some pre-processing in this step which includes the following: \n",
    "\n",
    "- Get rid of unwanted columns (active stage columns)  \n",
    "- Feature Engineering (driven from our hypothesis generation)    \n",
    "- Remove Duplicates  \n",
    "- Handle Missing Values  \n",
    "- Encode the Categorical Features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "dc35c3ec0000ff528251c4497e537bb021e1358f"
   },
   "outputs": [],
   "source": [
    "projects = projects.dropna()\n",
    "projects = projects[projects[\"currency\"] == \"USD\"]\n",
    "projects = projects[projects[\"state\"].isin([\"failed\", \"successful\"])]\n",
    "projects = projects.drop([\"backers\", \"ID\", \"currency\", \"country\", \"pledged\", \"usd pledged\", \"usd_pledged_real\", \"usd_goal_real\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e59e183c82ed706f53596f95e3271d8916818a23"
   },
   "source": [
    "**<font color=\"#5831bc\" face=\"Comic Sans MS\">Feature Engineering (Driven from Hypothesis Generation)</font>**  \n",
    "\n",
    "1. **Project Name / Description Features:** From our hypothesis, we suggested that how the project name or description is written may affect the success of the project. So we create some features related to project name. We dont have description of the project in this dataset, so we avoid that. \n",
    "\n",
    "  - Number of Words Used   \n",
    "  - Number of Characters Used   \n",
    "  - Number of Syllables Used (Difficult Words)  \n",
    "\n",
    "2. **Project Launched Date Features:** Also, we suggested that the project first launch can affect its success. So we create some date - time related features : \n",
    "\n",
    "    - Launched Day, Month, Quarter, Week  \n",
    "    - Total Duration of the Project  \n",
    "    - Was project launched on weekday or weekend   \n",
    "    - Was project launched on a holiday or regular day   \n",
    "    \n",
    "3. **Project Category Features**: These are more likely the high level features which provides the idea about the category / sub-category of the project. Also, we add some extra information with category such as the popularity of the category calculated from the total number of projects posted in that category.  \n",
    "\n",
    "    - Category Count and Sub Category Count : Generally how many projects are posted in those categories. This gives an idea if the project belongs to a more generic category or is more of a rare project  \n",
    "    - Category / Sub-Category Mean Goal : Generally what is the average goal set in those categories / sub-categories. This gives an idea if the project's goal is much higher or much lower than the standard mean goal of that category.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "f174fd01250940d0e200b6be2baaf335a9ec065a"
   },
   "outputs": [],
   "source": [
    "## feature engineering\n",
    "projects[\"syllable_count\"]   = projects[\"name\"].apply(lambda x: syllable_count(x))\n",
    "projects[\"launched_month\"]   = projects[\"launched\"].dt.month\n",
    "projects[\"launched_week\"]    = projects[\"launched\"].dt.week\n",
    "projects[\"launched_day\"]     = projects[\"launched\"].dt.weekday\n",
    "projects[\"is_weekend\"]       = projects[\"launched_day\"].apply(lambda x: 1 if x > 4 else 0)\n",
    "projects[\"num_words\"]        = projects[\"name\"].apply(lambda x: len(x.split()))\n",
    "projects[\"num_chars\"]        = projects[\"name\"].apply(lambda x: len(x.replace(\" \",\"\")))\n",
    "projects[\"duration\"]         = projects[\"deadline\"] - projects[\"launched\"]\n",
    "projects[\"duration\"]         = projects[\"duration\"].apply(lambda x: int(str(x).split()[0]))\n",
    "projects[\"state\"]            = projects[\"state\"].apply(lambda x: 1 if x==\"successful\" else 0)\n",
    "\n",
    "## label encoding the categorical features\n",
    "projects = pd.concat([projects, pd.get_dummies(projects[\"main_category\"])], axis = 1)\n",
    "le = LabelEncoder()\n",
    "for c in [\"category\", \"main_category\"]:\n",
    "    projects[c] = le.fit_transform(projects[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9952a1b16fae7368c9ec610a5bddf0ba42461a70"
   },
   "source": [
    "For Category and Main Category, I have used LabelEncoder, Some people may argue that LE may not be a perfect choice for this rather OneHot Encoder should be used. But In our use-case we are just trying to understand the effect of a column as a whole, so we can use label encoder. Now, we can generate the count / aggregation based features for main category and sub category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "641088d67396fb425dbd416fc3a3cff576639df7"
   },
   "outputs": [],
   "source": [
    "## Generate Count Features related to Category and Main Category\n",
    "t2 = projects.groupby(\"main_category\").agg({\"goal\" : \"mean\", \"category\" : \"sum\"})\n",
    "t1 = projects.groupby(\"category\").agg({\"goal\" : \"mean\", \"main_category\" : \"sum\"})\n",
    "t2 = t2.reset_index().rename(columns={\"goal\" : \"mean_main_category_goal\", \"category\" : \"main_category_count\"})\n",
    "t1 = t1.reset_index().rename(columns={\"goal\" : \"mean_category_goal\", \"main_category\" : \"category_count\"})\n",
    "projects = projects.merge(t1, on = \"category\")\n",
    "projects = projects.merge(t2, on = \"main_category\")\n",
    "\n",
    "projects[\"diff_mean_category_goal\"] = projects[\"mean_category_goal\"] - projects[\"goal\"]\n",
    "projects[\"diff_mean_category_goal\"] = projects[\"mean_main_category_goal\"] - projects[\"goal\"]\n",
    "\n",
    "projects = projects.drop([\"launched\", \"deadline\"], axis = 1)\n",
    "projects[[c for c in projects.columns if c != \"name\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9a14fdf30bed5ffa62fff533aa0b81eef91c2ba5"
   },
   "source": [
    "<div id=\"4\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">4. Modelling the Project Success </font>   </h2>\n",
    "\n",
    "Now, with all those features prepared we are ready to train our model. We will train a single random forest regression model for this task. There are ofcourse many other model's available as well such as lightgbm or xgboost, but in this kernel I am not focussing on evaluation metric rather the inisights from predictive modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56cb0dec7471191e8dfabda27da49cfa59c85ae3"
   },
   "outputs": [],
   "source": [
    "## define predictors and label \n",
    "label = projects.state\n",
    "features = [c for c in projects.columns if c not in [\"state\", \"name\"]]\n",
    "\n",
    "## prepare training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(projects[features], label, test_size = 0.025, random_state = 2)\n",
    "X_train1, y_train1 = X_train, y_train\n",
    "X_test1, y_test1 = X_test, y_test\n",
    "\n",
    "## train a random forest classifier \n",
    "model1 = RandomForestClassifier(n_estimators=50, random_state=0).fit(X_train1, y_train1)\n",
    "y_pred = model1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee4125b71c51f06716200674a39bf977125811c4"
   },
   "source": [
    "Now, we have a model which predicts the probability of a given project to be successful or not. In the next section we will interpret the model and its predictions. In other words, we will try to prove or disprove our hypothesis. \n",
    "\n",
    "<div id=\"5\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">5. Insights from Predictive Modelling </font>   </h2>\n",
    "\n",
    "- 5.1 Which are the most important features (relatively) of a project? ( **Relative Feature Importance** )       \n",
    "- 5.2 Which features have the biggest impact on the project success? ( **Permutation Importance** )      \n",
    "- 5.3 How does changes in those features affact the project success? ( **Partial Dependencies** )       \n",
    "- 5.4 Digging deeper into the decisions made by the model ( **SHAP values** )     \n",
    "\n",
    "<div id=\"5-1\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">5.1 Which are the most important features (relatively) of a project? (Relative Feature Importance)</font>   </h2>  \n",
    "\n",
    "In tree based models such as random forest, a number of decision tress are trained. During the tree building process, it can be computed how much each feature decreases the weighted impurity (or increases the information gain) in a tree. In random forest, the impurity decrease from each feature is averaged and the features are ranked according to this measure. This is called relative feature importance. The more an attribute is used to make key decisions with decision trees, the higher its relative importance. This indicates that the particular feature is one of the important features required to make accurate predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9b798297ca10a4e102c504673b9c87cf2523ca1d"
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(model1.feature_importances_, index = X_train.columns, columns=['importance'])\n",
    "feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "\n",
    "colors = [\"gray\"] * 9 + [\"green\"]*6\n",
    "trace1 = go.Bar(y = [x.title()+\"  \" for x in feature_importances.index[:15][::-1]], \n",
    "                x = feature_importances.importance[:15][::-1], \n",
    "                name=\"feature importance (relative)\",\n",
    "                marker=dict(color=colors, opacity=0.4), orientation = \"h\")\n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "layout = go.Layout(\n",
    "    margin=dict(l=400), width = 1000,\n",
    "    xaxis=dict(range=(0.0,0.15)),\n",
    "    title='Relative Feature Importance (Which Features are important to make predictions ?)',\n",
    "    barmode='group',\n",
    "    bargap=0.25\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "iplot(fig)\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "tag = \"<div> Most Important (Relative) : \"\n",
    "for feat in feature_importances.index[:10]:\n",
    "    tag += \"<span><font color='green'>\" +feat.title().replace(\"_\",\"\")+ \"</font> &nbsp;|&nbsp; </span>\" \n",
    "tag += \"<br>Least Important (Relative) : \"\n",
    "for feat in feature_importances.index[-15:]:\n",
    "    tag += \"<span><font color='red'>\" +feat.title().replace(\"_\",\"\")+ \"</font> &nbsp;|&nbsp; </span>\" \n",
    "tag += \"</div>\"\n",
    "display(HTML(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "00fb361772ed9697f41a38280b22b952099c5dea"
   },
   "source": [
    "**Inferences** \n",
    "> - From the graph, it is clear that the features which are important to predict the project success are: project goal, length of the project name, launched week, duration, and number of syllables present in the name. While the least important features are are mostly related to the project categories    \n",
    "> - **What does this mean for the project owner?** For someone who is willing to raise funds, they should consider evaluating the ideal project goal and duration. A high or a medium-high project goal may almost lead to the case of failure. Additionally, number of characters used in the project title will also affact if the project will be succeeded or failed.   \n",
    "> - **What does this mean for the company?** The company can identify the projects with high importance based on their meta - features such as length of the project.    \n",
    "\n",
    "By applying this approach, we primariy obtained the factors to look at a high level, But still we need to answer, what are the optimal values of these features. This will be answered when we apply other techniques in the next sections. Before moving on to those techniques, I wanted to explore a little more about relative feature importance using a graph theory perspective.  \n",
    "\n",
    "<br> <h2><font color=\"#5831bc\" face=\"Comic Sans MS\">A Graph Theory Perspective : Relative Feature Importances</font>   </h2>\n",
    "\n",
    "The idea of relative feature importance is very simple (more the times a feature appear in the decision tree splits, it is important) but many a times people forget an underlying important concept that these importances are \"relative\". This means that in comparison to other features what is the importance of a particular feature. \n",
    "\n",
    "But a question here is - Even if it is relative, what if some features from a set of features are removed, do we still obtain the same feature importances ? This problem can infact be formulated as graph problem. Consider a graph based structure, in which every feature (of the dataset) is a node, and the edges are defined between two features (nodes) if the two features appers in the *top 10 important features of the individual decision tree*. \n",
    "\n",
    "**But, What is the benefit?** Well, Some of the problems when viewed as network or graph problems can help to identify the solutions quickly and easily. For instance, using the graph properties one can identify which is the most important node of the network. A node having higher degree centrality (connections) indicates that the node is highly connected to the network. Which means that if a node is removed from the network, a large majority of the network will be disrupted. \n",
    "\n",
    "This idea can be applied to relative feature importances, a feature which is highly important, which appears in most of the decision tree's top 10 feature can be clearly identified from the feature importance network graph. If this feature is removed from the dataset then the predictions will be affacted. Let's plot this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bd7837f93c56a8f36a769461354a0d5cf992b278"
   },
   "outputs": [],
   "source": [
    "import itertools \n",
    "import networkx as nx \n",
    "G = nx.Graph()\n",
    "for tree_in_forest in model1.estimators_:\n",
    "    doc = {}\n",
    "    for i,key in enumerate(X_test1.columns):\n",
    "        doc[key] = tree_in_forest.feature_importances_[i]\n",
    "    sorted_doc = sorted(doc.items(), key=lambda kv: kv[1], reverse = True)[:10]\n",
    "    sorted_doc = [c for c in sorted_doc if c[0] != \"diff_mean_category_goal\"]\n",
    "    for i, j in itertools.product(sorted_doc, sorted_doc):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if i[1] >= 0.08 or j[1] >= 0.08:\n",
    "            if np.absolute(i[1] - j[1]) <= 0.05:\n",
    "                G.add_edge(i[0], j[0])\n",
    "\n",
    "k = dict(G.degree()).keys()\n",
    "v = dict(G.degree()).values()\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "ax = plt.subplot(111)\n",
    "plt.bar(k, v, width=0.80, color='#c4c4c4')\n",
    "plt.title(\"Degree Centrality of Features\")\n",
    "plt.ylabel(\"Degree Centrality\", fontsize=16)\n",
    "plt.xticks(rotation=15, fontsize=16)\n",
    "plt.yticks(color=\"white\")\n",
    "\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "\n",
    "def simpleaxis(ax):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.get_yaxis().tick_left()\n",
    "\n",
    "\n",
    "labels = {}    \n",
    "for node in G.nodes():\n",
    "    labels[node] = node\n",
    "        \n",
    "plt.axes([0.5, 0.4, 0.5, 0.5])\n",
    "pos = nx.spring_layout(G)\n",
    "plt.axis('off')\n",
    "nx.draw_networkx_nodes(G, pos, node_color = \"red\", node_size=20)\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=16, font_color='black')\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a0d1c238c15995b5987792dcb16547a8912dc9ba"
   },
   "source": [
    "> From the above plot, we can identify that the maximum degree centrality nodes are - duration, syllable count, goal etc. From the network perspective, it means that if we remove these nodes from the network (high degree centrality nodes), this will lead to network disruption. This is similar to what we obtained in the relative feature importance plot. The key takeaway from this graph is that one can not afford to ignore these features while optimizing any crowdfunding project. \n",
    "\n",
    "<div id=\"5-2\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">5.2 Which features have the biggest impact on the project success? (Permutation Importance) </font>   </h2>   \n",
    "\n",
    "In the last section, we mainly identified which the features at a very high level which are relatively important to the model outcome. In this section, we will go a little deeper and understand which features has the biggest impact on the model predictions (in absolute sense). One of the ways to identify such behaviour is to use permutation importance. \n",
    "\n",
    "The idea of permutation importance is very straightforward. After training a model, the model outcomes are obtained. The most important features for the model are the ones if the values of those feature are randomly shuffled then they lead to biggest drops in the model outcome accuracies. Let's look at the permutation importance of features of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ef0484c446818ff6fc6293d5d9a18159635270c9"
   },
   "outputs": [],
   "source": [
    "from eli5.sklearn import PermutationImportance\n",
    "import eli5\n",
    "perm = PermutationImportance(model1, random_state=1).fit(X_test, y_test)\n",
    "pi_df = eli5.explain_weights_df(perm, feature_names = X_test.columns.tolist())\n",
    "pi_df[\"color\"] = pi_df[\"weight\"].apply(lambda x : \"green\" if x > 0 else \"red\")\n",
    "\n",
    "data = [\n",
    "    go.Bar(\n",
    "        orientation = \"h\",\n",
    "        y = pi_df.feature[::-1],\n",
    "        x = pi_df.weight[::-1],\n",
    "        marker = dict(\n",
    "            opacity = 0.5,\n",
    "            color = pi_df.color[::-1]        ),\n",
    "        error_x = dict( type='data', color=\"#9fa3a3\",\n",
    "            array=list(pi_df[\"std\"][::-1]),\n",
    "            visible=True),\n",
    "        name = 'expenses'\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "layout = go.Layout(title=\"Permutation Importance\", height = 800, margin=dict(l=300))\n",
    "\n",
    "annotations = []\n",
    "for i, row in pi_df.iterrows():\n",
    "    dict(y=row.feature, x=row.weight, text=\"d\",\n",
    "                                  font=dict(family='Arial', size=14,\n",
    "                                  color='rgba(245, 246, 249, 1)'),\n",
    "                                  showarrow=False,)\n",
    "layout['annotations'] = annotations\n",
    "fig = go.Figure(data=data, layout = layout)\n",
    "iplot(fig, filename='base-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "712bd0333c35c5d96cef14f34068b3aa3b85a620"
   },
   "source": [
    "**Inferences** \n",
    "> - This is an interesting plot, We can observe that the features shown in top and in green are the most important as if their values are randomized then the outcome performance suffers.   \n",
    "> - We can observe that the top features are are the features which we mostly saw in the relative importance section, but using this graph we can quantify the amount of importance associated with them. And also obtain the ones which are least important, for example - launched week, if it was weekend or not etc.  \n",
    "\n",
    "With this method, we obtained the importance of a feature in a more absolute sense rathar than a relative sense. Let's assume that our feature space forms a majority of the universe. Now, it will be interesting to plot both permutation and relative feature importances and make some key observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "01b4dac0ca79cc70a4a03418027c6f726a7177c1"
   },
   "outputs": [],
   "source": [
    "imp_df = feature_importances.reset_index().rename(columns = {\"index\" : \"feature\"})\n",
    "combined_df = imp_df.merge(pi_df, on=\"feature\")\n",
    "\n",
    "trace0 = go.Scatter(\n",
    "    x = combined_df.importance,\n",
    "    y = combined_df.weight,\n",
    "    text = [v.title() if i < 16 else \"\" for i,v in enumerate(list(combined_df.feature)) ],\n",
    "    mode='markers+text',\n",
    "    textposition='top center',\n",
    "    marker=dict(\n",
    "        size = 10, color=\"red\", opacity=0.5,\n",
    "    ),\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=[0.034, 0.095],\n",
    "    y=[0.008, 0.020],\n",
    "    text=['Cluster of Features',\n",
    "          'Highly Important Features'],\n",
    "    mode='text',\n",
    ")\n",
    "\n",
    "data = [trace0]\n",
    "layout = go.Layout(title = \"Features : Relative Importance VS Permutation Importance\", \n",
    "                   showlegend = False, yaxis=dict(title=\"Permutation Importance (Feature Weight)\", showgrid=False),\n",
    "                   xaxis=dict(title=\"Feature Importance (Relative)\", showgrid=False))\n",
    "#                       shapes = [{ 'type': 'circle', 'xref': 'x', 'yref': 'y',\n",
    "#                                   'x0': 0.024, 'y0': 0.007, 'x1': 0.045, 'y1': 0.001,'opacity': 1.0,\n",
    "#                                   'line': { 'color': 'rgba(50, 171, 96, 1)', 'dash': 'dot',}},\n",
    "#                                { 'type': 'rect', 'x0': 0.065, 'y0': 0.019, 'x1': 0.12, 'y1': 0.0002,\n",
    "#                                 'line': { 'color': 'rgba(128, 0, 128, 1)' , 'dash' : 'dot' }}])\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "iplot(fig, filename='bubblechart-size-ref')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb6cac7e2675c3351036c6b18bbba3e2c4e65c30"
   },
   "source": [
    "**Inferences**  \n",
    "> - Very Interesting Insights can be obtained from the above plot, There are some features which showed up higher in the relative feature importance, but when we look at their permuatation importance we see that they are not important. (Though, permutation importance results cannot be reproduced exactly because of randomness, but when I first plotted this plot I observed that launched week and month had high feature importance but lower permutation importance.)  \n",
    "> - From this plot, we can again observe that our hypothesis is almost true, the project goal, duration, number of characters, number of words all are the most important features that one should look at while creating a new project page. \n",
    "\n",
    "### <font color=\"#5831bc\" face=\"Comic Sans MS\">Pressence of which keywords makes the biggest impact in the predictions?</font>  \n",
    "\n",
    "Using permutation importance, we can also evaluate which keywords makes the biggest impact in the model prediction. Let's train another model which also uses keywords used in the project name and observe the permutation importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "34c3b643d6dd52291282e8024f0f72ac4fb549f4"
   },
   "outputs": [],
   "source": [
    "def clean_name(x):\n",
    "    words = x.lower().split()\n",
    "    cln = [wrd for wrd in words if not wrd[0].isdigit()]\n",
    "    return \" \".join(cln)\n",
    "projects[\"cleaned_name\"] = projects[\"name\"].apply(lambda x : clean_name(x))\n",
    "\n",
    "## add text features : top 100\n",
    "vec = TfidfVectorizer(max_features=100, ngram_range=(1, 2), lowercase=True, stop_words=\"english\", min_df=6)\n",
    "X = vec.fit_transform(projects['cleaned_name'].values)\n",
    "\n",
    "## append to original dataframe\n",
    "vectors_df = pd.DataFrame(X.toarray(), columns=[\"_\"+xx for xx in vec.get_feature_names()])\n",
    "projects1_df = pd.concat([projects[features], vectors_df], axis=1)\n",
    "\n",
    "## train the model\n",
    "X_train, X_test, y_train, y_test = train_test_split(projects1_df, label, test_size = 0.25, random_state = 2)\n",
    "X_train2, y_train2 = X_train[:15000], y_train[:15000]\n",
    "X_test2, y_test2 = X_test[:1000], y_test[:1000]\n",
    "model2 = RandomForestClassifier(random_state=1).fit(X_train2, y_train2)\n",
    "y_pred = model2.predict(X_test2)\n",
    "\n",
    "####### Interpretation \n",
    "\n",
    "from plotly import tools\n",
    "\n",
    "perm = PermutationImportance(model2, random_state=1).fit(X_test2, y_test2)\n",
    "pi_df = eli5.explain_weights_df(perm, feature_names = X_test2.columns.tolist(), feature_filter=lambda x: x[0] == '_')\n",
    "pi_df[\"feature\"] = pi_df[\"feature\"].apply(lambda x : x[1:])\n",
    "highs = pi_df[pi_df.weight >= 0.001]\n",
    "med = pi_df[(pi_df.weight > -0.0005) & (pi_df.weight < 0.001)]\n",
    "lows = pi_df[pi_df.weight <= -0.0005]\n",
    "\n",
    "trace1 = go.Bar(\n",
    "        orientation = \"h\",\n",
    "        y = highs.feature[::-1],\n",
    "        x = highs.weight[::-1],\n",
    "        marker = dict(opacity = 0.4, color = \"green\" ), error_x = dict(type='data', color=\"#9fa3a3\", array=list(highs[\"std\"][::-1]), visible=True))\n",
    "trace2 = go.Bar(\n",
    "        orientation = \"h\",\n",
    "        y = med.feature[:15][::-1],\n",
    "        x = med.weight[:15][::-1],\n",
    "        marker = dict(opacity = 0.4, color = \"gray\"), error_x = dict(type='data', color=\"#9fa3a3\", array=list(med[\"std\"][:15][::-1]), visible=True))\n",
    "trace3 = go.Bar(\n",
    "        orientation = \"h\",\n",
    "        y = lows.feature,\n",
    "        x = lows.weight,\n",
    "        marker = dict(opacity = 0.4, color = \"red\"), error_x = dict(type='data', color=\"#9fa3a3\", array=list(lows[\"std\"][::-1]), visible=True))\n",
    "\n",
    "ttls = [\"Positive Impact\",\"\", \"Moderate + or - Impact\" ,\"\", \"Negative Impact\"]\n",
    "fig = tools.make_subplots(rows=1, cols=5, print_grid=False, subplot_titles = ttls)\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 3)\n",
    "fig.append_trace(trace3, 1, 5)\n",
    "\n",
    "fig['layout'].update(showlegend=False, title='Impact of Words Used in Project Name - Permutation Importance')\n",
    "iplot(fig, filename='simple-subplot-with-annotations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a430e132e170fd07d0bbe75e66c4fecf4ca32163"
   },
   "source": [
    "**Inferences**\n",
    "> - From the first plot, we can observe that there are a certain keywords which when used in the project name are likely to increase the probability success of a project. Example -  \"project\", \"film\", and \"community\". While on the other hand, keywords like \"game\", \"love\", \"fashion\" are likely to garner less attraction. This implies that crowdfunding projects related to games or entertainment such as love or fashion may not be very successful as compared to the ones related to art, design etc. \n",
    "\n",
    "**Note -** It is possible to get different results when run again, thus it is recommended to use this approach on a much bigger dataset. But ofcourse, its not a very big problem, atleast it gives an understanding about the words to focus on. \n",
    "\n",
    "<div id=\"5-3\"></div>\n",
    "<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">5.3 How does changes in features lead to changes in model outcome? (**Partial Dependencies**)  </font>   </h2>  \n",
    "\n",
    "<br>So far we have only talked about which features are most or least important from a pool of many features. For example, we observed that Project Goal, Project Duration, Number of Characters used etc are some of the important features related to project success. In this section, we will look at what are the specific values or ranges of features which leads to project success or failure. Specifically, we will observe that how making changes such as increasing or decreasing the values affect the model outcomes. These effects can be obtained by plotting the partial dependency plots of different features. \n",
    "\n",
    "### <font color=\"#5831bc\" face=\"Comic Sans MS\">Project Name - Features</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1aa8754c2564e8132803aa8df52133ad01836ef5"
   },
   "outputs": [],
   "source": [
    "def _plot_pdp(feature, pdp_color, fill_color):\n",
    "    plot_params = {\n",
    "        'title': feature.title() + ' - Partial Dependency Plot',\n",
    "        'title_fontsize': 15,\n",
    "        'subtitle': 'How changes in \"%s\" affects the model predictions' % feature.title(),\n",
    "        'subtitle_fontsize': 12,\n",
    "        'font_family': 'Calibri',\n",
    "        'xticks_rotation': 0,\n",
    "        'line_cmap': 'cool',\n",
    "        'zero_color': '#a2a5a0',\n",
    "        'zero_linewidth': 1.0,\n",
    "        'pdp_linewidth': 2.0,\n",
    "        'fill_alpha': 0.25,\n",
    "        'markersize': 5.5,\n",
    "        'pdp_hl_color': 'green',\n",
    "        'pdp_color': pdp_color,\n",
    "        'fill_color': fill_color,\n",
    "\n",
    "    }\n",
    "    pdp_goals = pdp.pdp_isolate(model=model1, dataset=X_test1, model_features=X_test1.columns, feature=feature)\n",
    "    pdp.pdp_plot(pdp_goals, feature, plot_params = plot_params)\n",
    "    plt.ylabel(\"Change in Model Predictions\");\n",
    "    plt.show();\n",
    "    \n",
    "cols_of_interest = ['num_words', 'num_chars', 'syllable_count',\n",
    "                    'duration', 'launched_month', 'launched_day',\n",
    "                    'category_count', 'main_category_count']\n",
    "\n",
    "_plot_pdp(cols_of_interest[0], \"#f442b3\", \"#efaad6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f70236d7010e20d5cfdd19a55061d0f77260a1a2"
   },
   "source": [
    "> We observe that the projects having fewer number of words (<= 3) in the name does not show any improvement in model success. However, if one start increasing the number of words in the project name, the corresponding model improvement also increases linearly. For all the projects having more than 10 words in the name, the model becomes saturate and shows similar predictions. Hence, the ideal word limit is somewhere around 7 - 10.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4cd4572ce866643a76ac7e444b86f0444111d51a"
   },
   "outputs": [],
   "source": [
    "_plot_pdp(cols_of_interest[1], \"#902fe0\", \"#c4a1e0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6373d99457f91e022d0b36ef23882b058e2da05e"
   },
   "source": [
    "> From the 2nd plot, we observe that if the total number of characters are less than 20, then model performance decreases than a normal value. Increasing the characters in the name linearly also increases the model performances.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "294700ab7829e52b7d9837d7e4f48d98d77a606a"
   },
   "outputs": [],
   "source": [
    "_plot_pdp(cols_of_interest[2], \"#5dcc2a\", \"#9dce86\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "380ddbc45b91a6f576d5c572eb8b9bf5366b4b45"
   },
   "source": [
    "> Change in Syllables does not show significant differences in model improvements. \n",
    "\n",
    "Let's also plot the interaction between number of words and characters used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ff5c7f0bdee9de5e58109a0d1909a7716e022414"
   },
   "outputs": [],
   "source": [
    "features_to_plot = ['num_words', 'num_chars']\n",
    "inter1 = pdp.pdp_interact(model1, X_test1, X_test1.columns, features_to_plot)\n",
    "\n",
    "plot_params = {\n",
    "    'title': 'PDP interactaction plot for NumWords and NumChars',\n",
    "    'subtitle': 'More red indicates better region',\n",
    "    'title_fontsize': 15,\n",
    "    'subtitle_fontsize': 12,\n",
    "    'contour_color':  'white',\n",
    "    'font_family': 'Calibri',\n",
    "    'cmap': 'rainbow',\n",
    "    'inter_fill_alpha': 0.6,\n",
    "    'inter_fontsize': 9,\n",
    "}\n",
    "\n",
    "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=features_to_plot, plot_type='contour', plot_params = plot_params)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ebc7ec9f505d6bc90aa82c925f2ce079ad43549"
   },
   "source": [
    "> From the above plot, it can be observed that about 40 - 65 characters and 10 - 14 words are the good numbers for the project name. \n",
    "\n",
    "### <font color=\"#5831bc\" face=\"Comic Sans MS\">Project Launched Day and Duration</font>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "61876508275a6643c657b9935b5f0ed412d405c9"
   },
   "outputs": [],
   "source": [
    "_plot_pdp(cols_of_interest[3], \"#ff0077\", \"#fcb3d5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f0927fed4abc2245927b82149a9ea62c753eadf"
   },
   "source": [
    "> For shorter project duration (less than 20 days), the chances that project will be successful are higher. However if the duration of a project is increased to say 60-90 days, it is less likely to acheive its goal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d40302253122b9b3952f3d8d935ee1b034884727"
   },
   "outputs": [],
   "source": [
    "_plot_pdp(cols_of_interest[4], \"#00ffbb\", \"#c2fcec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc4ab7b42f87a55ba7b6369a414f0d38ff4d67ef"
   },
   "source": [
    "> - We understood from the permutation importance that launched month has the less impact, which we can observe from partial dependency plots. But I just wanted to see are there any specific months in which the chances of project success are more. Looks like that towards the last quarter of the year (months 9 - 12), the success rate of projects is slightly higher while it is slightly lesser in quarter 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2d15800634b50945348e91dbdbb2bda9fb995fd2"
   },
   "outputs": [],
   "source": [
    "_plot_pdp(cols_of_interest[5], \"#ff9d00\", \"#f7d399\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4fcfb12e50581b3482a4c27fa5b2205879d6160"
   },
   "source": [
    "> For launch day, the model performance is lesser when launched day is friday - sunday as compared to monday - wednesdays. \n",
    "\n",
    "### <font color=\"#5831bc\" face=\"Comic Sans MS\">Project Main Category </font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1cbfc45c3caadc1cc32379ff76dee42f76425cd2"
   },
   "outputs": [],
   "source": [
    "_plot_pdp(cols_of_interest[7], \"#0800ff\", \"#cac9f2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c7ab98012a1484764042bb53b7f408caaea4462a"
   },
   "source": [
    "> From the feature definition, category count is a feature which act as the proxy of popularity of a project category. For example, if in Travel category a large number of projects are posted then its category_count will be higher so it is a popular category on Kickstarter. On the other hand, if in the Entertainment category, very rarely someone adds a project, its category_count will be lesser and so is its popularity. From the plot, we can observe that chances that a project will be successful will be higher if it belongs to a popular category. Also holds true for main category. \n",
    "\n",
    "### <font color=\"#5831bc\" face=\"Comic Sans MS\">How about specific categories ?</font>   \n",
    "\n",
    "By ploting the pdp_isolate graph we can also identify the effect of specific project categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "82249103c42372e3e0dad2225fdff0bba8df8c62"
   },
   "outputs": [],
   "source": [
    "pdp_category = pdp.pdp_isolate(model=model1, dataset=X_test1, model_features=X_test1.columns,\n",
    "                             feature=['Art', 'Comics', 'Crafts', 'Dance', 'Design', 'Fashion', 'Film & Video', \n",
    "                                      'Food', 'Games', 'Journalism', 'Music', 'Photography', 'Publishing', 'Technology', 'Theater'])\n",
    "fig, axes = pdp.pdp_plot(pdp_isolate_out = pdp_category, feature_name='Project Category', \n",
    "                         center=True, plot_lines=False, frac_to_plot=100, plot_pts_dist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5362707fa5938e3b0734fc1bff27d8a166a8ae1e"
   },
   "source": [
    "From the partial dependency plot for project category, we observe that the accuracy of model predicting the project success increases if it belongs to \"Music\", \"Comics\",  \"Theater\", or \"Dance\" categories.  It decreases if it belongs to \"Crafts\", \"Fashion Film & Video\". The same insights can be backed from the **actual predictions plot**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "dd4c721017eef9f32b25026db10c28cb4201f83a"
   },
   "outputs": [],
   "source": [
    "fig, axes, summary_df = info_plots.actual_plot_interact(\n",
    "    model=model1, X=X_test1,\n",
    "    features=['goal', ['Art', 'Comics', 'Crafts', 'Dance',\n",
    "       'Design', 'Fashion', 'Film & Video', 'Food', 'Games', 'Journalism',\n",
    "       'Music', 'Photography', 'Publishing', 'Technology', 'Theater']],\n",
    "    feature_names=['goal', 'Category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e9d71f8e15013612a431fc5291e129f1de09dd79"
   },
   "source": [
    "<div id=\"5-4\"></div>\n",
    "## <font color=\"#5831bc\" face=\"Comic Sans MS\">5.4 Understanding the decisions made by the Model (using SHAP) </font>  \n",
    "\n",
    "<br>In this section, We make the final predictions from our model and interpret them. For this purpose we will use of SHAP values which are the average of marginal contributions of individual feature values across all possible coalitions. **Let's try to understand this in laymen terms**, Consider a random project from the dataset with following features:\n",
    "\n",
    "* Title contains 8 words  \n",
    "* Title contains \"machine learning\"   \n",
    "* Project goal is US 10000 dollars   \n",
    "* Project is launched on a weekday   \n",
    "\n",
    "The trained model predicts that this project is likely to be successful with a probability of 75%. But, someone asks the question: **Why this project has the success probability of 75% not the 95% ?** To answer this question, we obtain the shap values for the prediction made by the model for this project. Shap values indicate the amount of increase or decrease in model outcome value from the average value of the predictions in the entire dataset. For example: \n",
    "\n",
    "* The average prediction value for this project would have been 45% without any model. \n",
    "* Due to the presence of 8 keywords in the project title the success probability is increased to 60%.  \n",
    "* Since, the title contains the bigram \"machine learning\", the success probability is further increased to 88%.   \n",
    "* Since the project is launched on a weekday, the success probability is increased by 2% to 90%  \n",
    "* However, since the project goal is too high (as compared to the average of the universe), the success probability is decreased from 90% to 75%.  \n",
    "\n",
    "Let's see the model predictions on the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d68c23a7c4cf45b4d5321ed91e61260c0a7fa4fa"
   },
   "outputs": [],
   "source": [
    "preds = model1.predict(X_test1)\n",
    "dict(Counter(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "58b116ce53695eefa38f1419972786a85da5e6ee"
   },
   "source": [
    "In a sample of around 6500 crowdfunding projects, Model predicts that about 4200 will be failed and only about 2300 will be successful. Now, we are interested to understand what is driving the success and failure of these projects.  Let's plot the individual feature effects on some of these predictions to make sense out of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "96cd7ce1c3f92115a5e5c95943243b874fe5d747"
   },
   "outputs": [],
   "source": [
    "import shap \n",
    "shap.initjs()\n",
    "data_for_prediction = X_test.iloc[1]\n",
    "explainer = shap.TreeExplainer(model1)\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, plot_cmap=[\"#f04e4e\",\"#6677f9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-output": false,
    "_uuid": "aa9695e56d9de20ab720dd254936c1a7e4fea7b9"
   },
   "source": [
    "> For this particular project, the prediction value is increased to 0.58 from the base value of 0.4178. This implies that presense of certain featuers and their corresponding values in this project makes it more likely to be successful. For instance, duration is 44, number of characters in project name are 27, and the difference in goal amount from the mean goal amount of the category is about 50K. These features increases the probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d9cd8dca690f879b268182f9df78bcf726dbd7dc"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "data_for_prediction = X_test.iloc[2]\n",
    "explainer = shap.TreeExplainer(model1)\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, plot_cmap=[\"#f2654f\",\"#7d4ff1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49f35c638cd21a2256a8c355f9c88dcadeb49421"
   },
   "source": [
    "> For this particular project, apart from number of characters, duration, the goal amount = 2000 also increases the probability from the base value of 0.4178 to 0.72. Not many features decreases the proabbility significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "86937701d4459fd3ae80a52b1daf749e2ab3e174"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "data_for_prediction = X_test.iloc[3]\n",
    "explainer = shap.TreeExplainer(model1)\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, plot_cmap=[\"#f79d2e\",\"#6677f9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f648b8e837d979788ab831bace239b4c286451e3"
   },
   "source": [
    "> For this project, the probability is not increased much as compared to other projects. Infact the probability is decreased due to the number of characters equal to 17, a high value of goal, and the duration of 29 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2e62105f0499a230f4a7c9030904c35ef7a19d5d"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "data_for_prediction = X_test.iloc[7]\n",
    "explainer = shap.TreeExplainer(model1)\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, plot_cmap=[\"#f04e4e\",\"#6677f9\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9ff48953ffa712749c18aae52ec63bd13569156"
   },
   "source": [
    "> For this project, duration of 25, small goal of 1500 significantly increases the project chances. However, less number of words (only 4), and difference from mean category goal amounts decreases the probability almost equally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "196852f4d96076754f0d6ee1b1dc006de994688d"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "data_for_prediction = X_test.iloc[10]\n",
    "explainer = shap.TreeExplainer(model1)\n",
    "shap_values = explainer.shap_values(data_for_prediction)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction, plot_cmap=[\"#f79d2e\",\"#812df7\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "45d5a4627fbda1c78305f32d826e2945c859cb04"
   },
   "source": [
    "> For this project, a large duration, presence of a particular category decreases the chances significantly. Not many features and the feature values are able to increase the project success chances. \n",
    "\n",
    "Now, we can aggreate the shap values for every feature for every prediction made by the model. This helps to understand an overall aggregated effect of the model features. Let's plt the summary plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "403932a4a4a2b2a5076b2c3092878a027771571c"
   },
   "outputs": [],
   "source": [
    "X_test_s = X_test.head(1000)\n",
    "shap_values = explainer.shap_values(X_test_s)\n",
    "shap.summary_plot(shap_values[1], X_test_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0bfb3ed95a7a864afedbd2bdd41243f5803daf9d"
   },
   "source": [
    "> We can observe that shap values is higher for the same set of features which we saw in relative feature importance and permutation importance. This confirms that features which greatly influence the project success are related to project features such as duration or goal. \n",
    "\n",
    "<div id=\"6\"></div>\n",
    "## <font color=\"#5831bc\" face=\"Comic Sans MS\">6. Final Conclusions</font>  \n",
    "\n",
    "After applying these different techniques we understood that there are certain factors which increases or decreases the chances of a project successfully able to raise the funds. Both from project owner's and the company's perspective it is important to set the optimal values of project goal and the duration. A large duration or a very large amount may not be able completely successful. At the same time it is important to choose right number of words and characters in the name of the project. For example, a project having very few or very large number of words and characters may become less intutive and less self explanatory. Similarly, the project category also plays a crutial role, There will be some categories on the platform in which total number of projects are very large, these are so called popular categories. The chances may be higher if the project is posted in a popular category rather than a rare category. \n",
    "\n",
    "In this kernel, I shared a general framework which I follow while solving any data science or analytics problem. This framework can be applied to many other different use-cases as well. The main focus of this kernel was different techniques related to machine learning explanability. Mainly, I used relative feature importance, permutation importance, partial dependecies, and shap values. In my opinion, here are some pros and cons of these techniques. Apart from these techniques, there are other alternatives as well (such as skater, lime). \n",
    "\n",
    "<table>\n",
    "    <tr><td>Technique</td> <td>Pros</td> <td>Cons</td></tr>\n",
    "    <tr><td>Feature Importance</td> <td>1. Available in almost all tree based models </td> <td>1. Does not gives the absolute ranking rather a relative ranking, hence it changes significantly with more number of features <br> 2. More accurate inferences requires more data, more iterations, more decision trees</td></tr>\n",
    "    <tr><td>Permutation Importance</td> <td>1. Easier, Fast to implement <br> 2. Does not require to train the model again and again </td> <td>1. Randomization of results <br> 2. Results on training can differ from that on test <br> 3. It only gives a single feature importance, but it is possible that a feature is not independent rather is important due to the presence of some other features. <br> 4. Requires a good amount of data to obtain good final values </td></tr>\n",
    "    <tr><td>Partial Dependencies</td> <td>1. Easier, faster to implement. <br> 2. Understanding of plots is simple yet powerful</td> <td>1. Very difficult to obtain the partial dependencies for more than 2 (or 3) features. </td></tr>\n",
    "    <tr><td>SHAP Values</td> <td>1. Backed by an excellent theory which provides a reasonable foundation</td> <td>1. Very slow to compute the aggregated effects <br> 2. It does not provide an exact solution rathar an approximate solution <br> 3. Somewhat difficult to interpret</td></tr>\n",
    "   </table>\n",
    "\n",
    "## References \n",
    "* https://www.kaggle.com/learn/machine-learning-explainability  \n",
    "* https://aiukraine.com/wp-content/uploads/2018/08/10_55-Kolbasin-Vladimir-How-to-explain-your-network_v2.pdf  \n",
    "* https://github.com/lopusz/awesome-interpretable-machine-learning  \n",
    "* https://christophm.github.io/interpretable-ml-book/index.html  \n",
    "* https://towardsdatascience.com/interpretable-machine-learning-with-xgboost-9ec80d148d27   \n",
    "* https://www.kaggle.com/cast42/lightgbm-model-explained-by-shap   \n",
    "* http://savvastjortjoglou.com/intrepretable-machine-learning-nfl-combine.html  \n",
    "* https://www.youtube.com/watch?v=s-yT5Is1G1A    \n",
    "* https://www.youtube.com/watch?v=vUqC8UPw9SU  \n",
    "* https://shirinsplayground.netlify.com/2018/07/explaining_ml_models_code_caret_iml/  \n",
    "\n",
    "Thanks for reading this notebook, if you liked it please upvote it and share the feedback. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
